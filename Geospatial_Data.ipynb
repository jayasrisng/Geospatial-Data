{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 8360460,
          "sourceType": "datasetVersion",
          "datasetId": 4968641
        }
      ],
      "dockerImageVersionId": 30698,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "Geospatial Data",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jayasrisng/Geospatial-Data/blob/main/Geospatial_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'user-tracking-dataset:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F4968641%2F8360460%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240508%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240508T223542Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D1853b505b586594949d282a97c9a2d6e1e09af220cac265008cd524f2d8cb0ccfb1c4565fab3d6155d11c7b5b7ad696febe28768a6ef63742a54863d8d195e9f4a8449fcc224afcea20626cd4b20de2c69558181babeddce5f6fe1e67b4a9e1c4706dc43b90c6bb3279ecbde5a7b125e8741f50dee2f12372750f291f6ede3ff9a2851cb0c62537bac7a56f67fa99b80db89b857ced33b1def584bcb7ae52bc57d1d7e430fca33116311c633bdde806f52ff49d5d93b0f933f68625297253f2f6c986734da2e2d96bbfa7c8631042beea58ef51f8daa271f7b8c3cc00d1b5ae7486145c158f4e1b1589f65f6f56bb3c8cb852a7c352174d55f086af7454dc819'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "W0_UEUV2WS3E"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "print(\"TensorFlow version:\", tf.__version__)"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2024-05-08T21:29:23.972477Z",
          "iopub.execute_input": "2024-05-08T21:29:23.973513Z",
          "iopub.status.idle": "2024-05-08T21:29:28.278077Z",
          "shell.execute_reply.started": "2024-05-08T21:29:23.973442Z",
          "shell.execute_reply": "2024-05-08T21:29:28.276805Z"
        },
        "trusted": true,
        "id": "BYzTuzQ7WS3J",
        "outputId": "4c8825e2-6497-43a3-f9d2-395686531f07"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "TensorFlow version: 2.15.0\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preparing Data"
      ],
      "metadata": {
        "id": "Et9s-MUjWS3K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "data_path = '/kaggle/input/user-tracking-dataset/combined_data.csv'\n",
        "data = pd.read_csv(data_path)\n",
        "\n",
        "# List of numeric feature columns, excluding 'Level Name' to exclude the non-numeric columns from your scaling process\n",
        "feature_columns = [\n",
        "    'Elapsed Time', 'Player Body Position X', 'Player Body Position Y',\n",
        "    'Player Body Position Z', 'Player Body Rotation X', 'Player Body Rotation Y',\n",
        "    'Player Body Rotation Z', 'Player Body Rotation W', 'Player Head Rotation X',\n",
        "    'Player Head Rotation Y', 'Player Head Rotation Z', 'Player Head Rotation W'\n",
        "]\n",
        "\n",
        "# Normalize the data\n",
        "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "scaled_data = scaler.fit_transform(data[feature_columns])\n",
        "scaled_data = np.array(scaled_data, dtype=np.float32)\n",
        "\n",
        "# Batch data\n",
        "batch_size = 32\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(scaled_data).shuffle(len(scaled_data)).batch(batch_size)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-08T21:50:13.520808Z",
          "iopub.execute_input": "2024-05-08T21:50:13.521212Z",
          "iopub.status.idle": "2024-05-08T21:50:14.056769Z",
          "shell.execute_reply.started": "2024-05-08T21:50:13.521185Z",
          "shell.execute_reply": "2024-05-08T21:50:14.055549Z"
        },
        "trusted": true,
        "id": "q7V5NUBKWS3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the GAN Architecture"
      ],
      "metadata": {
        "id": "3kbBuuAVWS3L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def make_generator_model(input_dim):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Input(shape=(input_dim,)),\n",
        "        tf.keras.layers.Dense(256, use_bias=False),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.LeakyReLU(),\n",
        "        tf.keras.layers.Dense(512, activation='relu'),\n",
        "        tf.keras.layers.Dense(np.prod([input_dim]), activation='tanh'),  # Adjust the output size as needed\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "def make_discriminator_model(input_dim):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Input(shape=(input_dim,)),\n",
        "        tf.keras.layers.Dense(512),\n",
        "        tf.keras.layers.LeakyReLU(),\n",
        "        tf.keras.layers.Dropout(0.3),\n",
        "        tf.keras.layers.Dense(256),\n",
        "        tf.keras.layers.LeakyReLU(),\n",
        "        tf.keras.layers.Dropout(0.3),\n",
        "        tf.keras.layers.Dense(1),\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "generator = make_generator_model(len(feature_columns))\n",
        "discriminator = make_discriminator_model(len(feature_columns))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-08T22:02:32.572443Z",
          "iopub.execute_input": "2024-05-08T22:02:32.572883Z",
          "iopub.status.idle": "2024-05-08T22:02:32.654825Z",
          "shell.execute_reply.started": "2024-05-08T22:02:32.572854Z",
          "shell.execute_reply": "2024-05-08T22:02:32.653355Z"
        },
        "trusted": true,
        "id": "6YkynTWtWS3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define Loss Functions and Optimizers"
      ],
      "metadata": {
        "id": "KpdOlj7VWS3M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "def discriminator_loss(real_output, fake_output):\n",
        "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
        "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "    total_loss = real_loss + fake_loss\n",
        "    return total_loss\n",
        "\n",
        "def generator_loss(fake_output):\n",
        "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
        "\n",
        "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-08T22:03:09.729777Z",
          "iopub.execute_input": "2024-05-08T22:03:09.730183Z",
          "iopub.status.idle": "2024-05-08T22:03:09.742845Z",
          "shell.execute_reply.started": "2024-05-08T22:03:09.73015Z",
          "shell.execute_reply": "2024-05-08T22:03:09.741693Z"
        },
        "trusted": true,
        "id": "5qg66orkWS3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training Loop"
      ],
      "metadata": {
        "id": "LXMcU1vyWS3N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def train_step(images):\n",
        "    noise = tf.random.normal([batch_size, len(feature_columns)])\n",
        "\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "        generated_images = generator(noise, training=True)\n",
        "\n",
        "        real_output = discriminator(images, training=True)\n",
        "        fake_output = discriminator(generated_images, training=True)\n",
        "\n",
        "        gen_loss = generator_loss(fake_output)\n",
        "        disc_loss = discriminator_loss(real_output, fake_output)\n",
        "\n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "\n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
        "\n",
        "def train(dataset, epochs):\n",
        "    for epoch in range(epochs):\n",
        "        for image_batch in dataset:\n",
        "            train_step(image_batch)\n",
        "        print(f'Epoch {epoch+1} Completed')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-08T22:03:29.960591Z",
          "iopub.execute_input": "2024-05-08T22:03:29.961027Z",
          "iopub.status.idle": "2024-05-08T22:03:29.972078Z",
          "shell.execute_reply.started": "2024-05-08T22:03:29.960993Z",
          "shell.execute_reply": "2024-05-08T22:03:29.971057Z"
        },
        "trusted": true,
        "id": "iWi_e-4YWS3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 50\n",
        "train(train_dataset, epochs)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-08T22:03:38.530032Z",
          "iopub.execute_input": "2024-05-08T22:03:38.530393Z",
          "iopub.status.idle": "2024-05-08T22:25:32.280578Z",
          "shell.execute_reply.started": "2024-05-08T22:03:38.530368Z",
          "shell.execute_reply": "2024-05-08T22:25:32.279656Z"
        },
        "trusted": true,
        "id": "YGbNzXNfWS3N",
        "outputId": "ca45141e-bdc2-4f2d-c4d3-de3b48ac5199"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Epoch 1 Completed\nEpoch 2 Completed\nEpoch 3 Completed\nEpoch 4 Completed\nEpoch 5 Completed\nEpoch 6 Completed\nEpoch 7 Completed\nEpoch 8 Completed\nEpoch 9 Completed\nEpoch 10 Completed\nEpoch 11 Completed\nEpoch 12 Completed\nEpoch 13 Completed\nEpoch 14 Completed\nEpoch 15 Completed\nEpoch 16 Completed\nEpoch 17 Completed\nEpoch 18 Completed\nEpoch 19 Completed\nEpoch 20 Completed\nEpoch 21 Completed\nEpoch 22 Completed\nEpoch 23 Completed\nEpoch 24 Completed\nEpoch 25 Completed\nEpoch 26 Completed\nEpoch 27 Completed\nEpoch 28 Completed\nEpoch 29 Completed\nEpoch 30 Completed\nEpoch 31 Completed\nEpoch 32 Completed\nEpoch 33 Completed\nEpoch 34 Completed\nEpoch 35 Completed\nEpoch 36 Completed\nEpoch 37 Completed\nEpoch 38 Completed\nEpoch 39 Completed\nEpoch 40 Completed\nEpoch 41 Completed\nEpoch 42 Completed\nEpoch 43 Completed\nEpoch 44 Completed\nEpoch 45 Completed\nEpoch 46 Completed\nEpoch 47 Completed\nEpoch 48 Completed\nEpoch 49 Completed\nEpoch 50 Completed\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1T_ggQRXWS3N"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}