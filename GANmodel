{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8360460,"sourceType":"datasetVersion","datasetId":4968641}],"dockerImageVersionId":30698,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport tensorflow as tf\n\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\ntf.get_logger().setLevel('ERROR')\nprint(\"TensorFlow version:\", tf.__version__)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-08T21:29:23.972477Z","iopub.execute_input":"2024-05-08T21:29:23.973513Z","iopub.status.idle":"2024-05-08T21:29:28.278077Z","shell.execute_reply.started":"2024-05-08T21:29:23.973442Z","shell.execute_reply":"2024-05-08T21:29:28.276805Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"TensorFlow version: 2.15.0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Preparing Data","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\n\ndata_path = '/kaggle/input/user-tracking-dataset/combined_data.csv'\ndata = pd.read_csv(data_path)\n\n# List of numeric feature columns, excluding 'Level Name' to exclude the non-numeric columns from your scaling process\nfeature_columns = [\n    'Elapsed Time', 'Player Body Position X', 'Player Body Position Y', \n    'Player Body Position Z', 'Player Body Rotation X', 'Player Body Rotation Y', \n    'Player Body Rotation Z', 'Player Body Rotation W', 'Player Head Rotation X', \n    'Player Head Rotation Y', 'Player Head Rotation Z', 'Player Head Rotation W'\n]\n\n# Normalize the data\nscaler = MinMaxScaler(feature_range=(-1, 1))\nscaled_data = scaler.fit_transform(data[feature_columns])\nscaled_data = np.array(scaled_data, dtype=np.float32)\n\n# Batch data\nbatch_size = 32\ntrain_dataset = tf.data.Dataset.from_tensor_slices(scaled_data).shuffle(len(scaled_data)).batch(batch_size)","metadata":{"execution":{"iopub.status.busy":"2024-05-08T21:50:13.520808Z","iopub.execute_input":"2024-05-08T21:50:13.521212Z","iopub.status.idle":"2024-05-08T21:50:14.056769Z","shell.execute_reply.started":"2024-05-08T21:50:13.521185Z","shell.execute_reply":"2024-05-08T21:50:14.055549Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"Define the GAN Architecture","metadata":{}},{"cell_type":"code","source":"import numpy as np\ndef make_generator_model(input_dim):\n    model = tf.keras.Sequential([\n        tf.keras.layers.Input(shape=(input_dim,)),\n        tf.keras.layers.Dense(256, use_bias=False),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.LeakyReLU(),\n        tf.keras.layers.Dense(512, activation='relu'),\n        tf.keras.layers.Dense(np.prod([input_dim]), activation='tanh'),  # Adjust the output size as needed\n    ])\n    return model\n\ndef make_discriminator_model(input_dim):\n    model = tf.keras.Sequential([\n        tf.keras.layers.Input(shape=(input_dim,)),\n        tf.keras.layers.Dense(512),\n        tf.keras.layers.LeakyReLU(),\n        tf.keras.layers.Dropout(0.3),\n        tf.keras.layers.Dense(256),\n        tf.keras.layers.LeakyReLU(),\n        tf.keras.layers.Dropout(0.3),\n        tf.keras.layers.Dense(1),\n    ])\n    return model\n\ngenerator = make_generator_model(len(feature_columns))\ndiscriminator = make_discriminator_model(len(feature_columns))","metadata":{"execution":{"iopub.status.busy":"2024-05-08T22:02:32.572443Z","iopub.execute_input":"2024-05-08T22:02:32.572883Z","iopub.status.idle":"2024-05-08T22:02:32.654825Z","shell.execute_reply.started":"2024-05-08T22:02:32.572854Z","shell.execute_reply":"2024-05-08T22:02:32.653355Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"Define Loss Functions and Optimizers","metadata":{}},{"cell_type":"code","source":"cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n\ndef discriminator_loss(real_output, fake_output):\n    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n    total_loss = real_loss + fake_loss\n    return total_loss\n\ndef generator_loss(fake_output):\n    return cross_entropy(tf.ones_like(fake_output), fake_output)\n\ngenerator_optimizer = tf.keras.optimizers.Adam(1e-4)\ndiscriminator_optimizer = tf.keras.optimizers.Adam(1e-4)","metadata":{"execution":{"iopub.status.busy":"2024-05-08T22:03:09.729777Z","iopub.execute_input":"2024-05-08T22:03:09.730183Z","iopub.status.idle":"2024-05-08T22:03:09.742845Z","shell.execute_reply.started":"2024-05-08T22:03:09.730150Z","shell.execute_reply":"2024-05-08T22:03:09.741693Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"Training Loop","metadata":{}},{"cell_type":"code","source":"@tf.function\ndef train_step(images):\n    noise = tf.random.normal([batch_size, len(feature_columns)])\n\n    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n        generated_images = generator(noise, training=True)\n\n        real_output = discriminator(images, training=True)\n        fake_output = discriminator(generated_images, training=True)\n\n        gen_loss = generator_loss(fake_output)\n        disc_loss = discriminator_loss(real_output, fake_output)\n\n    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n\n    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n\ndef train(dataset, epochs):\n    for epoch in range(epochs):\n        for image_batch in dataset:\n            train_step(image_batch)\n        print(f'Epoch {epoch+1} Completed')","metadata":{"execution":{"iopub.status.busy":"2024-05-08T22:03:29.960591Z","iopub.execute_input":"2024-05-08T22:03:29.961027Z","iopub.status.idle":"2024-05-08T22:03:29.972078Z","shell.execute_reply.started":"2024-05-08T22:03:29.960993Z","shell.execute_reply":"2024-05-08T22:03:29.971057Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"epochs = 50\ntrain(train_dataset, epochs)","metadata":{"execution":{"iopub.status.busy":"2024-05-08T22:03:38.530032Z","iopub.execute_input":"2024-05-08T22:03:38.530393Z","iopub.status.idle":"2024-05-08T22:25:32.280578Z","shell.execute_reply.started":"2024-05-08T22:03:38.530368Z","shell.execute_reply":"2024-05-08T22:25:32.279656Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"Epoch 1 Completed\nEpoch 2 Completed\nEpoch 3 Completed\nEpoch 4 Completed\nEpoch 5 Completed\nEpoch 6 Completed\nEpoch 7 Completed\nEpoch 8 Completed\nEpoch 9 Completed\nEpoch 10 Completed\nEpoch 11 Completed\nEpoch 12 Completed\nEpoch 13 Completed\nEpoch 14 Completed\nEpoch 15 Completed\nEpoch 16 Completed\nEpoch 17 Completed\nEpoch 18 Completed\nEpoch 19 Completed\nEpoch 20 Completed\nEpoch 21 Completed\nEpoch 22 Completed\nEpoch 23 Completed\nEpoch 24 Completed\nEpoch 25 Completed\nEpoch 26 Completed\nEpoch 27 Completed\nEpoch 28 Completed\nEpoch 29 Completed\nEpoch 30 Completed\nEpoch 31 Completed\nEpoch 32 Completed\nEpoch 33 Completed\nEpoch 34 Completed\nEpoch 35 Completed\nEpoch 36 Completed\nEpoch 37 Completed\nEpoch 38 Completed\nEpoch 39 Completed\nEpoch 40 Completed\nEpoch 41 Completed\nEpoch 42 Completed\nEpoch 43 Completed\nEpoch 44 Completed\nEpoch 45 Completed\nEpoch 46 Completed\nEpoch 47 Completed\nEpoch 48 Completed\nEpoch 49 Completed\nEpoch 50 Completed\n","output_type":"stream"}]}]}