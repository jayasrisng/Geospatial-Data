# -*- coding: utf-8 -*-
"""Copy of Geospatial Data

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14cSu-Pi_KCHq2AatjYyqtJiviTDJaN1u
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES
# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.

import os
import sys
from tempfile import NamedTemporaryFile
from urllib.request import urlopen
from urllib.parse import unquote, urlparse
from urllib.error import HTTPError
from zipfile import ZipFile
import tarfile
import shutil

CHUNK_SIZE = 40960
DATA_SOURCE_MAPPING = 'user-tracking-dataset:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F4968641%2F8360460%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240508%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240508T233933Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D06f4211cbcabe1494492a81d42afc757ef296c557a6f052b84a9b6eeebd3085487479dec57c7683b0441d14a69f9b032a6e55ac7c79e932263ef342650480f76d353c1d1b9011fd7dc09eaf6e20832e8d6c7d431485681962992d67e4896cba4c26af34e850def04319c24f734a1032b564c688401875554a1bf0ee5f5440576199756fe9af1adbb30620b02f82f263feb3762f020555b6c26800496b60d43b07d1b383de852ff6191158e8fe2c0b04137fcba0223ce2ea7a0c77bd7c95a34cee9394fde5310f076410006bdafe39692f4b1a26e43d48c4ac78447261c107376cea0d6ce96cba5ef65d19a4de5aba4580d6d55b74fa407d6925ec451f5b1e388'

KAGGLE_INPUT_PATH='/kaggle/input'
KAGGLE_WORKING_PATH='/kaggle/working'
KAGGLE_SYMLINK='kaggle'

!umount /kaggle/input/ 2> /dev/null
shutil.rmtree('/kaggle/input', ignore_errors=True)
os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)
os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)

try:
  os.symlink(KAGGLE_INPUT_PATH, os.path.join("..", 'input'), target_is_directory=True)
except FileExistsError:
  pass
try:
  os.symlink(KAGGLE_WORKING_PATH, os.path.join("..", 'working'), target_is_directory=True)
except FileExistsError:
  pass

for data_source_mapping in DATA_SOURCE_MAPPING.split(','):
    directory, download_url_encoded = data_source_mapping.split(':')
    download_url = unquote(download_url_encoded)
    filename = urlparse(download_url).path
    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)
    try:
        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:
            total_length = fileres.headers['content-length']
            print(f'Downloading {directory}, {total_length} bytes compressed')
            dl = 0
            data = fileres.read(CHUNK_SIZE)
            while len(data) > 0:
                dl += len(data)
                tfile.write(data)
                done = int(50 * dl / int(total_length))
                sys.stdout.write(f"\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded")
                sys.stdout.flush()
                data = fileres.read(CHUNK_SIZE)
            if filename.endswith('.zip'):
              with ZipFile(tfile) as zfile:
                zfile.extractall(destination_path)
            else:
              with tarfile.open(tfile.name) as tarfile:
                tarfile.extractall(destination_path)
            print(f'\nDownloaded and uncompressed: {directory}')
    except HTTPError as e:
        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')
        continue
    except OSError as e:
        print(f'Failed to load {download_url} to path {destination_path}')
        continue

print('Data source import complete.')

import os
import tensorflow as tf

os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
tf.get_logger().setLevel('ERROR')
print("TensorFlow version:", tf.__version__)

"""Preparing Data"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler

data_path = '/kaggle/input/user-tracking-dataset/combined_data.csv'
data = pd.read_csv(data_path)

# List of numeric feature columns, excluding 'Level Name' to exclude the non-numeric columns from your scaling process
feature_columns = [
    'Elapsed Time', 'Player Body Position X', 'Player Body Position Y',
    'Player Body Position Z', 'Player Body Rotation X', 'Player Body Rotation Y',
    'Player Body Rotation Z', 'Player Body Rotation W', 'Player Head Rotation X',
    'Player Head Rotation Y', 'Player Head Rotation Z', 'Player Head Rotation W'
]

# Normalize the data
scaler = MinMaxScaler(feature_range=(-1, 1))
scaled_data = scaler.fit_transform(data[feature_columns])
scaled_data = np.array(scaled_data, dtype=np.float32)

# Batch data
batch_size = 32
train_dataset = tf.data.Dataset.from_tensor_slices(scaled_data).shuffle(len(scaled_data)).batch(batch_size)

"""Define the GAN Architecture"""

import numpy as np
def make_generator_model(input_dim):
    model = tf.keras.Sequential([
        tf.keras.layers.Input(shape=(input_dim,)),
        tf.keras.layers.Dense(256, use_bias=False),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.LeakyReLU(),
        tf.keras.layers.Dense(512, activation='relu'),
        tf.keras.layers.Dense(np.prod([input_dim]), activation='tanh'),  # Adjust the output size as needed
    ])
    return model

def make_discriminator_model(input_dim):
    model = tf.keras.Sequential([
        tf.keras.layers.Input(shape=(input_dim,)),
        tf.keras.layers.Dense(512),
        tf.keras.layers.LeakyReLU(),
        tf.keras.layers.Dropout(0.3),
        tf.keras.layers.Dense(256),
        tf.keras.layers.LeakyReLU(),
        tf.keras.layers.Dropout(0.3),
        tf.keras.layers.Dense(1),
    ])
    return model

generator = make_generator_model(len(feature_columns))
discriminator = make_discriminator_model(len(feature_columns))

"""Define Loss Functions and Optimizers"""

cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)

def discriminator_loss(real_output, fake_output):
    real_loss = cross_entropy(tf.ones_like(real_output), real_output)
    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)
    total_loss = real_loss + fake_loss
    return total_loss

def generator_loss(fake_output):
    return cross_entropy(tf.ones_like(fake_output), fake_output)

generator_optimizer = tf.keras.optimizers.Adam(1e-4)
discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)

"""Training Loop"""

@tf.function
def train_step(images):
    noise = tf.random.normal([batch_size, len(feature_columns)])

    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:
        generated_images = generator(noise, training=True)

        real_output = discriminator(images, training=True)
        fake_output = discriminator(generated_images, training=True)

        gen_loss = generator_loss(fake_output)
        disc_loss = discriminator_loss(real_output, fake_output)

    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)
    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)

    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))
    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))

def train(dataset, epochs):
    for epoch in range(epochs):
        for image_batch in dataset:
            train_step(image_batch)
        print(f'Epoch {epoch+1} Completed')

epochs = 50
train(train_dataset, epochs)

"""Generate New Synthetic Data"""

def generate_synthetic_data(generator, num_samples):
    noise = tf.random.normal([num_samples, len(feature_columns)])
    generated_data = generator(noise, training=False)
    return generated_data.numpy()

num_samples = 10
new_data = generate_synthetic_data(generator, num_samples)

new_data_rescaled = scaler.inverse_transform(new_data)

""" Dimensionality Reduction"""

original_data_encoded = pd.get_dummies(original_data)

plt.figure(figsize=(10, 8))
sns.heatmap(original_data_encoded.corr(), annot=True, fmt=".2f", cmap='coolwarm')
plt.title('Correlation Matrix - With Encoded Categorical Data')
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

corr = original_data_encoded.corr()

plt.figure(figsize=(20, 15))

mask = np.triu(np.ones_like(corr, dtype=bool))

sns.heatmap(corr, mask=mask, cmap='coolwarm', vmax=1, vmin=-1, center=0,
            square=True, linewidths=.5, cbar_kws={"shrink": .5}, annot=True, fmt=".2f",
            annot_kws={"size": 8})

plt.xticks(rotation=90)
plt.yticks(rotation=0)
plt.show()

corr.to_csv('correlation_matrix.csv')

import numpy as np
import pandas as pd

num_samples = 100
num_features = 3
synthetic_array = np.random.rand(num_samples, num_features)

column_names = ['feature1', 'feature2', 'feature3']
generated_data = pd.DataFrame(synthetic_array, columns=column_names)

print(generated_data.head())

output_path = 'synthetic_data.csv'
generated_data.to_csv(output_path, index=False)

print(f"Data saved successfully to {output_path}")

import pandas as pd

data_path = '/kaggle/input/user-tracking-dataset/combined_data.csv'

data = pd.read_csv(data_path)

print(data.head())

from sklearn.preprocessing import MinMaxScaler

feature_columns = [col for col in data.columns if data[col].dtype in [float, int]]

scaler = MinMaxScaler(feature_range=(-1, 1))
scaled_data = scaler.fit_transform(data[feature_columns])

scaled_data_df = pd.DataFrame(scaled_data, columns=feature_columns)
print(scaled_data_df.head())

noise_dim = 100
generator = make_generator_model(input_dim=noise_dim)
discriminator = make_discriminator_model(input_dim=noise_dim)

import pandas as pd
from sklearn.preprocessing import MinMaxScaler

data_path = '/kaggle/input/user-tracking-dataset/combined_data.csv'
data = pd.read_csv(data_path)

numerical_columns = data.select_dtypes(include=[np.number]).columns
data = data[numerical_columns]

data.fillna(data.mean(), inplace=True)

scaler = MinMaxScaler(feature_range=(-1, 1))
scaled_data = scaler.fit_transform(data)
scaled_data_df = pd.DataFrame(scaled_data, columns=numerical_columns)

data = data.dropna(axis=1, how='all')

data.fillna(data.mean(), inplace=True)

if data.isnull().any().any():
    print("There are still NaNs in the dataset.")
else:
    print("No more NaNs in the dataset.")

import tensorflow as tf

def make_generator_model(input_dim):
    model = tf.keras.Sequential([
        tf.keras.layers.Dense(128, activation='relu', input_shape=(input_dim,)),
        tf.keras.layers.Dense(256, activation='relu'),
        tf.keras.layers.Dense(input_dim, activation='tanh')
    ])
    return model

def make_discriminator_model(input_dim):
    model = tf.keras.Sequential([
        tf.keras.layers.Dense(256, activation='relu', input_shape=(input_dim,)),
        tf.keras.layers.Dense(128, activation='relu'),
        tf.keras.layers.Dense(1, activation='sigmoid')
    ])
    return model

scaler = MinMaxScaler(feature_range=(-1, 1))
scaled_data = scaler.fit_transform(data)
scaled_data_df = pd.DataFrame(scaled_data, columns=data.columns)

input_dim = scaled_data_df.shape[1]

generator = make_generator_model(input_dim)
discriminator = make_discriminator_model(input_dim)

def train_gan(generator, discriminator, dataset, epochs):
    for epoch in range(epochs):
        for batch_data in dataset:
            pass
        print(f'Epoch {epoch + 1}/{epochs} completed.')
    print("Training completed.")

train_dataset = tf.data.Dataset.from_tensor_slices(scaled_data_df.values).shuffle(1024).batch(32)
train_gan(generator, discriminator, train_dataset, epochs=50)

def generate_synthetic_data(generator, input_dim, num_examples):
    noise = tf.random.normal([num_examples, input_dim])
    synthetic_data = generator(noise, training=False).numpy()
    return synthetic_data

# Generate synthetic data
synthetic_data = generate_synthetic_data(generator, input_dim, num_examples=100)
synthetic_df = pd.DataFrame(synthetic_data, columns=scaled_data_df.columns)
print(synthetic_df.head())

synthetic_df = pd.DataFrame(synthetic_data, columns=scaled_data_df.columns)

output_csv_path = 'new_synthetic_data.csv'

synthetic_df.to_csv(output_csv_path, index=False)

print(f"Synthetic data saved successfully to {output_csv_path}")

import tensorflow as tf

def train_step(real_data):
    real_data = tf.cast(real_data, tf.float32)

    noise = tf.random.normal([real_data.shape[0], input_dim], dtype=tf.float32)
    fake_data = generator(noise, training=True)

    mixed_data = tf.concat([real_data, fake_data], axis=0)

    labels = tf.concat([tf.ones((real_data.shape[0], 1), dtype=tf.float32),  # Real data
                        tf.zeros((real_data.shape[0], 1), dtype=tf.float32)],  # Fake data
                       axis=0)

    d_loss = discriminator.train_on_batch(mixed_data, labels)

    misleading_labels = tf.ones((real_data.shape[0], 1), dtype=tf.float32)
    g_loss = generator.train_on_batch(noise, misleading_labels)

    return d_loss, g_loss

import numpy as np

scaled_data = scaled_data.astype(np.float32)
train_dataset = tf.data.Dataset.from_tensor_slices(scaled_data).shuffle(1024).batch(32)